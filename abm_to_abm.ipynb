{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79845107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time\n",
    "from scipy.stats import uniform, norm, multivariate_normal\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from agent_based_model import load_data, preprocess_data, set_initial_values, main_function\n",
    "from main_pool import Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13506bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC class for parameter estimation\n",
    "class ABC_Agent:\n",
    "    \"\"\"\n",
    "    Approximate Bayesian Computation for agent-based model parameter estimation\n",
    "    \"\"\"\n",
    "    def __init__(self, observed_data, data_path=\"./chelyabinsk_10/\", days=range(1, 100)):\n",
    "        \"\"\"\n",
    "        Initialize ABC class\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        observed_data : pandas.DataFrame\n",
    "            Observed epidemic data\n",
    "        data_path : str\n",
    "            Path to the population data files\n",
    "        days : range\n",
    "            Range of days to simulate\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.data_path = data_path\n",
    "        self.days = days\n",
    "        \n",
    "        # define strain keys\n",
    "        self.strains_keys = ['H1N1', 'H3N2', 'B']\n",
    "        \n",
    "        # prepare data only once\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        self.data, self.households, self.dict_school_id = load_data(data_path)\n",
    "        self.data, self.households, self.dict_school_id = preprocess_data(self.data, self.households, self.dict_school_id)\n",
    "        self.dict_school_len = [len(self.dict_school_id[i]) for i in self.dict_school_id.keys()]\n",
    "        \n",
    "        # store history matching results\n",
    "        self.hm_results = None\n",
    "    \n",
    "    def run_simulation(self, params):\n",
    "        \"\"\"\n",
    "        Run a simulation with given parameters using Main class\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Dictionary with model parameters (alpha, lmbd)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        simulation_output : pandas.DataFrame\n",
    "            Time series of simulated data\n",
    "        \"\"\"\n",
    "        alpha = params['alpha']\n",
    "        lmbd = params['lmbd']\n",
    "        \n",
    "        # create a temporary directory for results\n",
    "        sim_dir = f\"temp_sim_{np.random.randint(0, 100000)}/\"\n",
    "        if not os.path.exists(sim_dir):\n",
    "            os.makedirs(sim_dir)\n",
    "        \n",
    "        try:\n",
    "            pool = Main(\n",
    "                strains_keys=self.strains_keys,\n",
    "                infected_init=[10, 0, 0],\n",
    "                alpha=[alpha, alpha, alpha],\n",
    "                lmbd=lmbd\n",
    "            )\n",
    "            \n",
    "            num_runs = 5\n",
    "            # configure runs\n",
    "            pool.runs_params(\n",
    "                num_runs=num_runs,\n",
    "                days=[1, len(self.days)],\n",
    "                data_folder=self.data_path\n",
    "            )\n",
    "            \n",
    "            # define age groups\n",
    "            pool.age_groups_params(\n",
    "                age_groups=['0-10', '11-17', '18-59', '60-150'],\n",
    "                vaccined_fraction=[0, 0, 0, 0]\n",
    "            )\n",
    "            \n",
    "            # run simulation\n",
    "            pool.start(with_seirb=True)\n",
    "            \n",
    "            # load ALL results from different seeds\n",
    "            all_results = []\n",
    "            for run_number in range(num_runs):\n",
    "                results_path = os.path.join(pool.results_dir, f\"prevalence_seed_{run_number}.csv\")\n",
    "                if os.path.exists(results_path):\n",
    "                    sim_results = pd.read_csv(results_path, sep='\\t')\n",
    "                    sim_results['run'] = run_number\n",
    "                    all_results.append(sim_results)\n",
    "    \n",
    "            combined_results = pd.concat(all_results, ignore_index=True)\n",
    "            return combined_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            # clean up temporary directory\n",
    "            import shutil\n",
    "            if os.path.exists(sim_dir):\n",
    "                shutil.rmtree(sim_dir)\n",
    "    \n",
    "    def calculate_distance(self, sim_data):\n",
    "        \"\"\"\n",
    "        Calculate distance between simulated and observed data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sim_data : pandas.DataFrame\n",
    "            Simulated data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        distance : float\n",
    "            Distance metric (MSE)\n",
    "        \"\"\"\n",
    "        if sim_data is None:\n",
    "            return np.inf\n",
    "            \n",
    "        try:\n",
    "            # use H1N1 strain for comparison\n",
    "            min_len = min(len(self.observed_data), len(sim_data))\n",
    "            obs = self.observed_data['H1N1'].values[:min_len]\n",
    "            sim = sim_data['H1N1'].values[:min_len]\n",
    "            \n",
    "            # mean squared error\n",
    "            distance = np.mean((obs - sim)**2)\n",
    "            return distance\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance: {e}\")\n",
    "            return np.inf\n",
    "    \n",
    "    def history_matching(self, prior_ranges, n_samples=100, epsilon=0.1, adaptive=True, accept_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Perform history matching to find plausible parameter regions\n",
    "        \"\"\"\n",
    "        print(f\"Running history matching with {n_samples} samples...\")\n",
    "    \n",
    "        # generate samples from prior ranges (which we know from prior knowledge)\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = {}\n",
    "            for param, (min_val, max_val) in prior_ranges.items():\n",
    "                sample[param] = uniform.rvs(loc=min_val, scale=max_val-min_val)\n",
    "            samples.append(sample)\n",
    "    \n",
    "        # run simulations and calculate distances (between generated and observed data)\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.run_simulation(sample)\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "        \n",
    "            # store trajectory data\n",
    "            result_dict = {\n",
    "                \"alpha\": sample[\"alpha\"],\n",
    "                \"lmbd\": sample[\"lmbd\"],\n",
    "                \"distance\": distance\n",
    "            }\n",
    "        \n",
    "            # add trajectory to results dictionary\n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "        \n",
    "            results.append(result_dict)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "    \n",
    "        # print distance statistics for debugging\n",
    "        print(f\"Distance stats: min={results_df['distance'].min()}, max={results_df['distance'].max()}, mean={results_df['distance'].mean()}\")\n",
    "    \n",
    "        # filter results (accept)\n",
    "        if adaptive:\n",
    "            n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "            accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "        else:\n",
    "            accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "    \n",
    "        print(f\"Accepted {len(accepted)} parameter sets\")\n",
    "        \n",
    "        # store results for other methods to use\n",
    "        self.hm_results = accepted\n",
    "    \n",
    "        return accepted\n",
    "\n",
    "    \n",
    "    def rejection_abc(self, n_samples=100, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Perform ABC rejection sampling based on history matching results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_samples : int\n",
    "            Number of parameter samples to evaluate\n",
    "        epsilon : float\n",
    "            Threshold for accepting parameter sets\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accepted_params : pandas.DataFrame\n",
    "            Accepted parameter sets\n",
    "        \"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"Must run history_matching before rejection_abc\")\n",
    "            \n",
    "        print(f\"Running ABC rejection with {n_samples} samples from History Matching...\")\n",
    "        \n",
    "        # use history matching results to define parameter ranges\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # sample from history matching parameter space\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # randomly select a parameter set from history matching results\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            # add small perturbation to create a new sample\n",
    "            alpha_perturb = uniform.rvs(loc=-0.05, scale=0.1)  # +- 0.05\n",
    "            lmbd_perturb = uniform.rvs(loc=-0.05, scale=0.1)   # +- 0.05\n",
    "            \n",
    "            alpha = np.clip(hm_sample['alpha'] + alpha_perturb, alpha_min, alpha_max)\n",
    "            lmbd = np.clip(hm_sample['lmbd'] + lmbd_perturb, lmbd_min, lmbd_max)\n",
    "            \n",
    "            samples.append({\"alpha\": alpha, \"lmbd\": lmbd})\n",
    "            \n",
    "        # run simulations and calculate distances\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.run_simulation(sample)\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"alpha\": sample[\"alpha\"],\n",
    "                \"lmbd\": sample[\"lmbd\"],\n",
    "                \"distance\": distance\n",
    "            }\n",
    "            \n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "                \n",
    "            results.append(result_dict)\n",
    "            \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # select only parameters with distance < epsilon\n",
    "        accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "        if len(accepted) == 0:\n",
    "            print(f\"No samples accepted at epsilon = {epsilon}. Taking best samples.\")\n",
    "            accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "        \n",
    "        return accepted\n",
    "    \n",
    "    def annealing_abc(self, n_samples=100, initial_epsilon=1e-3, final_epsilon=1e-5, cooling_steps=3):\n",
    "        \"\"\"\n",
    "        Perform ABC with simulated annealing using history matching results\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_samples : int\n",
    "            Number of parameter samples per step\n",
    "        initial_epsilon : float\n",
    "            Initial acceptance threshold\n",
    "        final_epsilon : float\n",
    "            Final acceptance threshold\n",
    "        cooling_steps : int\n",
    "            Number of annealing steps\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accepted_params : pandas.DataFrame\n",
    "            Accepted parameter sets\n",
    "        \"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"Must run history_matching before annealing_abc\")\n",
    "            \n",
    "        print(f\"Running ABC annealing with {cooling_steps} cooling steps...\")\n",
    "        \n",
    "        # calculate epsilon values for each step\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, cooling_steps)\n",
    "        \n",
    "        # get parameter bounds from history matching\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # initial samples from history matching results\n",
    "        current_samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # randomly select a parameter set from history matching results\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            current_samples.append({\n",
    "                \"alpha\": hm_sample[\"alpha\"], \n",
    "                \"lmbd\": hm_sample[\"lmbd\"]\n",
    "            })\n",
    "    \n",
    "        # run annealing process\n",
    "        for step, epsilon in enumerate(epsilons):\n",
    "            print(f\"Annealing step {step+1}/{cooling_steps}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate current samples\n",
    "            results = []\n",
    "            for sample in tqdm(current_samples):\n",
    "                sim_data = self.run_simulation(sample)\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                \n",
    "                result_dict = {\n",
    "                    \"alpha\": sample[\"alpha\"],\n",
    "                    \"lmbd\": sample[\"lmbd\"],\n",
    "                    \"distance\": distance\n",
    "                }\n",
    "                \n",
    "                if sim_data is not None:\n",
    "                    result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "                \n",
    "                results.append(result_dict)\n",
    "            \n",
    "            # filter accepted samples\n",
    "            results_df = pd.DataFrame(results)\n",
    "            accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "            \n",
    "            if len(accepted) == 0:\n",
    "                print(f\"No samples accepted at epsilon = {epsilon}. Taking best samples.\")\n",
    "                accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "            \n",
    "            # generate new samples for next iteration\n",
    "            if step < cooling_steps - 1:\n",
    "                # calculate mean and variance of accepted parameters\n",
    "                alpha_mean = accepted[\"alpha\"].mean()\n",
    "                lmbd_mean = accepted[\"lmbd\"].mean()\n",
    "                \n",
    "                # mke absolutely sure the variances are strictly positive\n",
    "                alpha_var = accepted[\"alpha\"].var()\n",
    "                if np.isnan(alpha_var) or alpha_var <= 1e-6:\n",
    "                    alpha_var = 1e-4  # set a safe minimum value\n",
    "                \n",
    "                lmbd_var = accepted[\"lmbd\"].var()\n",
    "                if np.isnan(lmbd_var) or lmbd_var <= 1e-6:\n",
    "                    lmbd_var = 1e-4  # set a safe minimum value\n",
    "                \n",
    "                # generate new samples from normal distribution around accepted values\n",
    "                current_samples = []\n",
    "                for _ in range(n_samples):\n",
    "                    try:\n",
    "                        alpha = norm.rvs(loc=alpha_mean, scale=np.sqrt(alpha_var))\n",
    "                        lmbd = norm.rvs(loc=lmbd_mean, scale=np.sqrt(lmbd_var))\n",
    "                    except ValueError:\n",
    "                        # fallback if there's still an error\n",
    "                        perturb_scale = 0.05  # 5% perturbation\n",
    "                        alpha = alpha_mean + np.random.uniform(-perturb_scale, perturb_scale) * (alpha_max - alpha_min)\n",
    "                        lmbd = lmbd_mean + np.random.uniform(-perturb_scale, perturb_scale) * (lmbd_max - lmbd_min)\n",
    "                    \n",
    "                    # ensure parameters are within bounds from history matching\n",
    "                    alpha = max(alpha_min, min(alpha, alpha_max))\n",
    "                    lmbd = max(lmbd_min, min(lmbd, lmbd_max))\n",
    "                    \n",
    "                    current_samples.append({\n",
    "                        \"alpha\": alpha,\n",
    "                        \"lmbd\": lmbd\n",
    "                    })\n",
    "        \n",
    "        # return final accepted samples\n",
    "        return accepted\n",
    "\n",
    "    \n",
    "    def smc_abc(self, n_particles=100, n_populations=3, initial_epsilon=1e-3, final_epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Perform ABC Sequential Monte Carlo using history matching results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_particles : int\n",
    "            Number of particles\n",
    "        n_populations : int\n",
    "            Number of SMC iterations\n",
    "        initial_epsilon : float\n",
    "            Initial acceptance threshold\n",
    "        final_epsilon : float\n",
    "            Final acceptance threshold\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        posterior : pandas.DataFrame\n",
    "            Posterior distribution with weights\n",
    "        \"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"Must run history_matching before smc_abc\")\n",
    "            \n",
    "        print(f\"Running ABC-SMC with {n_populations} populations...\")\n",
    "        \n",
    "        # calculate epsilon sequence\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, n_populations)\n",
    "        \n",
    "        # get parameter bounds from history matching\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # initialize first population from history matching results\n",
    "        particles = []\n",
    "        for _ in range(n_particles):\n",
    "            # randomly select a parameter set from history matching\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            particles.append({\n",
    "                \"alpha\": hm_sample[\"alpha\"], \n",
    "                \"lmbd\": hm_sample[\"lmbd\"]\n",
    "            })\n",
    "        \n",
    "        # equal weights for first population\n",
    "        weights = np.ones(n_particles) / n_particles\n",
    "        \n",
    "        # run SMC process\n",
    "        for t in range(n_populations):\n",
    "            epsilon = epsilons[t]\n",
    "            print(f\"SMC Population {t+1}/{n_populations}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate particles and calculate distances\n",
    "            distances = []\n",
    "            trajectories = []\n",
    "            for particle in tqdm(particles):\n",
    "                sim_data = self.run_simulation(particle)\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                distances.append(distance)\n",
    "                if sim_data is not None:\n",
    "                    trajectories.append(sim_data[\"H1N1\"].copy())\n",
    "                else:\n",
    "                    trajectories.append(None)\n",
    "            \n",
    "            # update weights based on epsilon\n",
    "            new_weights = np.zeros(n_particles)\n",
    "            for i, distance in enumerate(distances):\n",
    "                if distance < epsilon:\n",
    "                    new_weights[i] = weights[i]\n",
    "            \n",
    "            # normalize weights\n",
    "            if np.sum(new_weights) > 0:\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            else:\n",
    "                print(f\"No particles accepted at epsilon = {epsilon}. Taking best particles.\")\n",
    "                # take best 10% particles\n",
    "                sorted_indices = np.argsort(distances)\n",
    "                for i in range(max(1, int(n_particles * 0.1))):\n",
    "                    new_weights[sorted_indices[i]] = 1.0\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            \n",
    "            # calculate effective sample size\n",
    "            ESS = 1.0 / np.sum(new_weights**2)\n",
    "            print(f\"Effective Sample Size: {ESS:.2f}\")\n",
    "            \n",
    "            # resample if needed\n",
    "            if ESS < n_particles / 2 or t == n_populations - 1:\n",
    "                # resample based on weights\n",
    "                indices = np.random.choice(n_particles, size=n_particles, p=new_weights)\n",
    "                resampled_particles = [particles[i] for i in indices]\n",
    "                resampled_trajectories = [trajectories[i] for i in indices]\n",
    "                particles = resampled_particles\n",
    "                trajectories = resampled_trajectories\n",
    "                weights = np.ones(n_particles) / n_particles\n",
    "            else:\n",
    "                weights = new_weights\n",
    "            \n",
    "            # if not final iteration, perturb particles\n",
    "            if t < n_populations - 1:\n",
    "                # calculate kernel covariance\n",
    "                alpha_values = np.array([p[\"alpha\"] for p in particles])\n",
    "                lmbd_values = np.array([p[\"lmbd\"] for p in particles])\n",
    "                \n",
    "                params = np.vstack([alpha_values, lmbd_values]).T\n",
    "                cov = np.cov(params.T) + np.eye(2) * 1e-6  # add small diagonal for stability\n",
    "                \n",
    "                # perturb particles\n",
    "                new_particles = []\n",
    "                for i, particle in enumerate(particles):\n",
    "                    accepted = False\n",
    "                    attempts = 0\n",
    "                    while not accepted and attempts < 100:\n",
    "                        attempts += 1\n",
    "                        # multivariate normal perturbation\n",
    "                        perturbation = multivariate_normal.rvs(mean=[0, 0], cov=cov)\n",
    "                        new_alpha = particle[\"alpha\"] + perturbation[0]\n",
    "                        new_lmbd = particle[\"lmbd\"] + perturbation[1]\n",
    "                        \n",
    "                        # check if within history matching bounds\n",
    "                        alpha_in_bounds = alpha_min <= new_alpha <= alpha_max\n",
    "                        lmbd_in_bounds = lmbd_min <= new_lmbd <= lmbd_max\n",
    "                        \n",
    "                        if alpha_in_bounds and lmbd_in_bounds:\n",
    "                            accepted = True\n",
    "                            new_particles.append({\"alpha\": new_alpha, \"lmbd\": new_lmbd})\n",
    "                    \n",
    "                    # if couldn't generate valid particle after max attempts, keep original\n",
    "                    if not accepted:\n",
    "                        new_particles.append(particle)\n",
    "                \n",
    "                particles = new_particles\n",
    "        \n",
    "        # return final particles and weights\n",
    "        final_results = []\n",
    "        for i, particle in enumerate(particles):\n",
    "            final_results.append({\n",
    "                \"alpha\": particle[\"alpha\"],\n",
    "                \"lmbd\": particle[\"lmbd\"],\n",
    "                \"weight\": weights[i],\n",
    "                \"distance\": distances[i],\n",
    "                \"trajectory\": trajectories[i]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0ba5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating synthetic data and plotting\n",
    "def generate_synthetic_data(alpha=0.78, lmbd=0.4, days=range(1, 100)):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with the actual agent-based model.\n",
    "    The synthetic data generated should be created in one sample.\n",
    "    \"\"\"\n",
    "    # create a Main instance with known parameters\n",
    "    pool = Main(\n",
    "        strains_keys=['H1N1', 'H3N2', 'B'],\n",
    "        infected_init=[10, 0, 0], \n",
    "        alpha=[alpha, alpha, alpha],\n",
    "        lmbd=lmbd\n",
    "    )\n",
    "    \n",
    "    # configure the simulation\n",
    "    num_runs = 1\n",
    "    pool.runs_params(\n",
    "        num_runs=num_runs,\n",
    "        days=[1, len(days)],\n",
    "        data_folder='chelyabinsk_10'\n",
    "    )\n",
    "    \n",
    "    # configure age groups\n",
    "    pool.age_groups_params(\n",
    "        age_groups=['0-10', '11-17', '18-59', '60-150'],\n",
    "        vaccined_fraction=[0, 0, 0, 0]\n",
    "    )\n",
    "    \n",
    "    # run the simulation\n",
    "    pool.start(with_seirb=True)\n",
    "    \n",
    "    # load results\n",
    "    results_path = os.path.join(pool.results_dir, \"prevalence_seed_0.csv\")\n",
    "    data = pd.read_csv(results_path, sep='\\t')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149b163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(observed_data, abc_results, method_name, n_trajectories=5):\n",
    "    \"\"\"\n",
    "    Plot parameter posterior and time series comparison\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    if len(abc_results) == 0:\n",
    "        axes[0].text(0.5, 0.5, f\"No accepted parameter sets for {method_name}\", \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "        axes[1].text(0.5, 0.5, f\"No accepted parameter sets for {method_name}\",\n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "    else:\n",
    "        # plot parameter posterior\n",
    "        if 'weight' in abc_results.columns:\n",
    "            # for SMC with weights\n",
    "            scatter = axes[0].scatter(abc_results[\"alpha\"], abc_results[\"lmbd\"], \n",
    "                                  s=abc_results[\"weight\"]*100, alpha=0.6)\n",
    "        else:\n",
    "            # for methods without weights\n",
    "            scatter = axes[0].scatter(abc_results[\"alpha\"], abc_results[\"lmbd\"], alpha=0.6)\n",
    "    \n",
    "        axes[0].set_title(f\"Parameter posterior - {method_name}\")\n",
    "        axes[0].set_xlabel(\"Alpha\")\n",
    "        axes[0].set_ylabel(\"Lambda\")\n",
    "    \n",
    "        # plot trajectories from abc_results\n",
    "        n_plot = min(n_trajectories, len(abc_results))\n",
    "        for i in range(n_plot):\n",
    "            traj = abc_results.iloc[i][\"trajectory\"]\n",
    "            axes[1].plot(traj, alpha=0.6, label=f\"Trajectory {i+1}\")\n",
    "    \n",
    "        # plot time series (of observed)\n",
    "        axes[1].plot(np.tile(observed_data[\"H1N1\"], 5), label=\"Observed\", color=\"black\", linestyle=\"--\")\n",
    "        axes[1].axvline(100, color='b', alpha = 0.6, linestyle=\"--\")\n",
    "        axes[1].axvline(200, color='b', alpha = 0.6, linestyle=\"--\")\n",
    "        axes[1].axvline(300, color='b', alpha = 0.6, linestyle=\"--\")\n",
    "        axes[1].axvline(400, color='b', alpha = 0.6, linestyle=\"--\")\n",
    "        \n",
    "        axes[1].set_title(\"Time series comparison\")\n",
    "        axes[1].set_xlabel(\"Time\")\n",
    "        axes[1].set_ylabel(\"Infected\")\n",
    "        axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter ranges for priors\n",
    "prior_ranges = {\n",
    "    \"alpha\": (0.5, 0.9),   # susceptibility\n",
    "    \"lmbd\": (0.1, 0.5)     # transmissibility\n",
    "}\n",
    "    \n",
    "# generate synthetic data with known parameters for testing\n",
    "true_alpha = 0.78\n",
    "true_lmbd = 0.4\n",
    "observed_data = generate_synthetic_data(alpha=true_alpha, lmbd=true_lmbd)\n",
    "    \n",
    "# create ABC object\n",
    "abc = ABC_Agent(observed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run History Matching\n",
    "hm_results = abc.history_matching(prior_ranges, n_samples=100, epsilon=0.1)#1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f710cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(observed_data, hm_results, \"History Matching\", n_trajectories=len(hm_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC Rejection\n",
    "rejection_results = abc.rejection_abc(n_samples=20, epsilon=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(observed_data, rejection_results, \"ABC Rejection\", len(rejection_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC Annealing\n",
    "annealing_results = abc.annealing_abc(n_samples=10, \n",
    "                                     initial_epsilon=1e-3, \n",
    "                                     final_epsilon=1e-5, \n",
    "                                     cooling_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(observed_data, annealing_results, \"ABC Annealing\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a68740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC SMC\n",
    "smc_results = abc.smc_abc(n_particles=10, \n",
    "                         n_populations=3, \n",
    "                         initial_epsilon=1e-3, \n",
    "                         final_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(observed_data, smc_results, \"ABC SMC\", len(smc_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the methods\n",
    "methods = {\n",
    "        'history_matching': hm_results['distance'],\n",
    "        'rejection': rejection_results['distance'] if len(rejection_results) > 0 else np.nan,\n",
    "        'annealing': annealing_results['distance'] if len(annealing_results) > 0 else np.nan,\n",
    "        'smc': smc_results['distance']\n",
    "    }\n",
    "    \n",
    "    # create comparison dataframe\n",
    "comparison = pd.DataFrame(methods)\n",
    "print(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
