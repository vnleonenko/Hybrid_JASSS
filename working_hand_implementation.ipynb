{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7971a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import uniform, norm, multivariate_normal\n",
    "from agent_based_model import load_data, preprocess_data\n",
    "from main_pool import Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af778a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABM_tuned:\n",
    "    \"\"\"Agent-Based model parameter estimation class\"\"\"\n",
    "    \n",
    "    def __init__(self, observed_data, data_path=\"./chelyabinsk_10/\", days=range(1, 100)): # days to predict\n",
    "        \"\"\"Initializion\"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.data_path = data_path\n",
    "        self.days = days\n",
    "        self.strains_keys = ['H1N1', 'H3N2', 'B']\n",
    "        \n",
    "        # we read data only once\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        self.data, self.households, self.dict_school_id = load_data(data_path)\n",
    "        self.data, self.households, self.dict_school_id = preprocess_data(\n",
    "            self.data, self.households, self.dict_school_id)\n",
    "        \n",
    "        # store history matching results\n",
    "        self.hm_results = None\n",
    "    \n",
    "    def simulator_function(self, alpha, lmbd):\n",
    "        \"\"\"Run ABM simulation with given parameters\"\"\"\n",
    "        # temporary directory for simulation results\n",
    "        sim_dir = f\"temp_sim_{np.random.randint(0, 100000)}/\"\n",
    "        if not os.path.exists(sim_dir):\n",
    "            os.makedirs(sim_dir)\n",
    "        \n",
    "        try:\n",
    "            pool = Main(\n",
    "                strains_keys=self.strains_keys,\n",
    "                infected_init=[10, 0, 0],\n",
    "                alpha=[float(alpha), float(alpha), float(alpha)],\n",
    "                lmbd=float(lmbd)\n",
    "            )\n",
    "             \n",
    "            pool.runs_params(\n",
    "                num_runs=1,\n",
    "                days=[1, len(self.days)],\n",
    "                data_folder=self.data_path\n",
    "            )\n",
    "            \n",
    "            pool.age_groups_params(\n",
    "                age_groups=['0-10', '11-17', '18-59', '60-150'],\n",
    "                vaccined_fraction=[0, 0, 0, 0]\n",
    "            )\n",
    "            \n",
    "            pool.start(with_seirb=True)\n",
    "            \n",
    "            # load results of the simulation\n",
    "            results_path = os.path.join(pool.results_dir, \"prevalence_seed_0.csv\")\n",
    "            if os.path.exists(results_path):\n",
    "                sim_results = pd.read_csv(results_path, sep='\\t')\n",
    "                return sim_results\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            if os.path.exists(sim_dir):\n",
    "                shutil.rmtree(sim_dir)\n",
    "    \n",
    "    def calculate_distance(self, sim_data):\n",
    "        \"\"\" distance between simulated and observed data\"\"\"\n",
    "        if sim_data is None:\n",
    "            return np.inf\n",
    "        \n",
    "        try:\n",
    "            min_len = min(len(self.observed_data), len(sim_data))\n",
    "            obs = self.observed_data['H1N1'].values[:min_len]\n",
    "            sim = sim_data['H1N1'].values[:min_len]\n",
    "            \n",
    "            # mean squared error\n",
    "            distance = np.mean((obs - sim)**2)\n",
    "            return distance\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance: {e}\")\n",
    "            return np.inf\n",
    "    \n",
    "    def history_matching(self, prior_ranges, n_samples=100, epsilon=3, adaptive=False, accept_ratio=0.2):\n",
    "        \"\"\"history matching to find plausible parameter regions uses uniform random sampling within prior parameter ranges\"\"\"\n",
    "        print(f\"Running history matching with {n_samples} samples...\") # n_samples means how many times we randomly pick the parameters\n",
    "\n",
    "        # generate samples from prior ranges\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = {}\n",
    "            for param, (min_val, max_val) in prior_ranges.items():\n",
    "                sample[param] = uniform.rvs(loc=min_val, scale=max_val-min_val)\n",
    "            samples.append(sample)\n",
    "\n",
    "        # run simulations and calculate distances\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.simulator_function(sample[\"alpha\"], sample[\"lmbd\"])\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "    \n",
    "            # trajectory data\n",
    "            result_dict = {\n",
    "            \"alpha\": sample[\"alpha\"],\n",
    "            \"lmbd\": sample[\"lmbd\"],\n",
    "            \"distance\": distance\n",
    "            }\n",
    "        \n",
    "            # add trajectory to results dictionary\n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "    \n",
    "            results.append(result_dict)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # distance statistics\n",
    "        #print(f\"Distance stats: min={results_df['distance'].min()}, max={results_df['distance'].max()}, mean={results_df['distance'].mean()}\")\n",
    "\n",
    "        # filter results \n",
    "        if adaptive: # by acceptance ratio\n",
    "            n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "            accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "        else: # fixed threshold\n",
    "            accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "\n",
    "        print(f\"Accepted {len(accepted)} parameter sets\")\n",
    "    \n",
    "        # results for other methods to use\n",
    "        self.hm_results = accepted\n",
    "\n",
    "        return accepted\n",
    "\n",
    "    \n",
    "    def rejection_abc(self, n_samples=100, epsilon=1e-5, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\" ABC rejection sampling based on history matching results\"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"One must run history_matching before rejection_abc\")\n",
    "            \n",
    "        print(f\"Running ABC rejection with {n_samples} samples from History Matching...\")\n",
    "        \n",
    "        # we use history matching results to define parameter ranges\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # sample from history matching parameter space\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # randomly select a parameter set from history matching results\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            # add small perturbation to create a new sample\n",
    "            alpha_perturb = uniform.rvs(loc=-0.05, scale=0.1)  # +- 0.05\n",
    "            lmbd_perturb = uniform.rvs(loc=-0.05, scale=0.1)   # +- 0.05\n",
    "            \n",
    "            alpha = np.clip(hm_sample['alpha'] + alpha_perturb, alpha_min, alpha_max)\n",
    "            lmbd = np.clip(hm_sample['lmbd'] + lmbd_perturb, lmbd_min, lmbd_max)\n",
    "            \n",
    "            samples.append({\"alpha\": alpha, \"lmbd\": lmbd})\n",
    "            \n",
    "        # run simulations and calculate distances\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.simulator_function(sample[\"alpha\"], sample[\"lmbd\"])\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"alpha\": sample[\"alpha\"],\n",
    "                \"lmbd\": sample[\"lmbd\"],\n",
    "                \"distance\": distance\n",
    "            }\n",
    "            \n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "                \n",
    "            results.append(result_dict)\n",
    "            \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if adaptive: # by acceptance ratio\n",
    "            n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "            accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "        else: # FIXED THRESHOLD: select only parameters with distance < epsilon\n",
    "            if len(accepted) == 0:\n",
    "                print(f\"No samples accepted at epsilon = {epsilon}. Taking 10% of the best samples.\")\n",
    "                accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "            else:\n",
    "                accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "    \n",
    "            #accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "        \n",
    "        return accepted\n",
    "    \n",
    "    def annealing_abc(self, n_samples=100, initial_epsilon=1e-3, final_epsilon=1e-5, cooling_steps=3, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\" ABC simulated annealing using history matching results\"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"One must run history_matching before annealing_abc\")\n",
    "            \n",
    "        print(f\"Running ABC annealing with {cooling_steps} cooling steps...\")\n",
    "        \n",
    "        # epsilon values for each step\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, cooling_steps)\n",
    "        \n",
    "        # parameter bounds from history matching\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # initial samples from history matching results\n",
    "        current_samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # randomly select a parameter set from history matching results\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            current_samples.append({\n",
    "                \"alpha\": hm_sample[\"alpha\"], \n",
    "                \"lmbd\": hm_sample[\"lmbd\"]\n",
    "            })\n",
    "    \n",
    "        # annealing process\n",
    "        for step, epsilon in enumerate(epsilons):\n",
    "            print(f\"Annealing step {step+1}/{cooling_steps}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate current samples\n",
    "            results = []\n",
    "            for sample in tqdm(current_samples):\n",
    "                sim_data = self.simulator_function(sample[\"alpha\"], sample[\"lmbd\"])\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                \n",
    "                result_dict = {\n",
    "                    \"alpha\": sample[\"alpha\"],\n",
    "                    \"lmbd\": sample[\"lmbd\"],\n",
    "                    \"distance\": distance\n",
    "                }\n",
    "                \n",
    "                if sim_data is not None:\n",
    "                    result_dict[\"trajectory\"] = sim_data[\"H1N1\"].copy()\n",
    "                \n",
    "                results.append(result_dict)\n",
    "            \n",
    "            # Filter accepted samples\n",
    "            results_df = pd.DataFrame(results)\n",
    "            #accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "            \n",
    "            #if len(accepted) == 0:\n",
    "                #print(f\"No samples accepted at epsilon = {epsilon}. Taking best samples.\")\n",
    "                #accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "\n",
    "            if adaptive: # by acceptance ratio\n",
    "                n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "                accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "            else: # FIXED THRESHOLD: select only parameters with distance < epsilon\n",
    "                if len(accepted) == 0:\n",
    "                    print(f\"No samples accepted at epsilon = {epsilon}. Taking 10% of the best samples.\")\n",
    "                    accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "                else:\n",
    "                    accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "    \n",
    "            \n",
    "            # new samples for next iteration\n",
    "            if step < cooling_steps - 1:\n",
    "                # mean of accepted parameters\n",
    "                alpha_mean = accepted[\"alpha\"].mean()\n",
    "                lmbd_mean = accepted[\"lmbd\"].mean()\n",
    "                \n",
    "                # ensure the variances are strictly positive\n",
    "                alpha_var = accepted[\"alpha\"].var()\n",
    "                if np.isnan(alpha_var) or alpha_var <= 1e-6:\n",
    "                    alpha_var = 1e-4  # minimum value\n",
    "                \n",
    "                lmbd_var = accepted[\"lmbd\"].var()\n",
    "                if np.isnan(lmbd_var) or lmbd_var <= 1e-6:\n",
    "                    lmbd_var = 1e-4  # minimum value\n",
    "                \n",
    "                # new samples from normal distribution around accepted values\n",
    "                current_samples = []\n",
    "                for _ in range(n_samples):\n",
    "                    try:\n",
    "                        alpha = norm.rvs(loc=alpha_mean, scale=np.sqrt(alpha_var))\n",
    "                        lmbd = norm.rvs(loc=lmbd_mean, scale=np.sqrt(lmbd_var))\n",
    "                    except ValueError:\n",
    "                        # fallback if there's an error\n",
    "                        perturb_scale = 0.05  # 5% perturbation\n",
    "                        alpha = alpha_mean + np.random.uniform(-perturb_scale, perturb_scale) * (alpha_max - alpha_min)\n",
    "                        lmbd = lmbd_mean + np.random.uniform(-perturb_scale, perturb_scale) * (lmbd_max - lmbd_min)\n",
    "                    \n",
    "                    # ensure parameters are within bounds from history matching\n",
    "                    alpha = max(alpha_min, min(alpha, alpha_max))\n",
    "                    lmbd = max(lmbd_min, min(lmbd, lmbd_max))\n",
    "                    \n",
    "                    current_samples.append({\n",
    "                        \"alpha\": alpha,\n",
    "                        \"lmbd\": lmbd\n",
    "                    })\n",
    "        \n",
    "        return accepted\n",
    "    \n",
    "    def smc_abc(self, n_particles=100, n_populations=3, initial_epsilon=1e-3, final_epsilon=1e-5, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\"ABC Sequential Monte Carlo using history matching results\"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"One must run history_matching before smc_abc\")\n",
    "            \n",
    "        print(f\"Running ABC-SMC with {n_populations} populations...\")\n",
    "        \n",
    "        # epsilon sequence\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, n_populations)\n",
    "        \n",
    "        # parameter bounds from history matching\n",
    "        alpha_min = self.hm_results['alpha'].min()\n",
    "        alpha_max = self.hm_results['alpha'].max()\n",
    "        lmbd_min = self.hm_results['lmbd'].min()\n",
    "        lmbd_max = self.hm_results['lmbd'].max()\n",
    "        \n",
    "        # first population from history matching results\n",
    "        particles = []\n",
    "        for _ in range(n_particles):\n",
    "            # randomly select a parameter set from history matching\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            particles.append({\n",
    "                \"alpha\": hm_sample[\"alpha\"], \n",
    "                \"lmbd\": hm_sample[\"lmbd\"]\n",
    "            })\n",
    "        \n",
    "        # equal weights for first population\n",
    "        weights = np.ones(n_particles) / n_particles\n",
    "        \n",
    "        # SMC process\n",
    "        for t in range(n_populations):\n",
    "            epsilon = epsilons[t]\n",
    "            print(f\"SMC Population {t+1}/{n_populations}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate particles and calculate distances\n",
    "            distances = []\n",
    "            trajectories = []\n",
    "            for particle in tqdm(particles):\n",
    "                sim_data = self.simulator_function(particle[\"alpha\"], particle[\"lmbd\"])\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                distances.append(distance)\n",
    "                if sim_data is not None:\n",
    "                    trajectories.append(sim_data[\"H1N1\"].copy())\n",
    "                else:\n",
    "                    trajectories.append(None)\n",
    "            \n",
    "            # update weights based on epsilon\n",
    "            new_weights = np.zeros(n_particles)\n",
    "            for i, distance in enumerate(distances):\n",
    "                if distance < epsilon:\n",
    "                    new_weights[i] = weights[i]\n",
    "            \n",
    "            # normalize weights\n",
    "            if np.sum(new_weights) > 0:\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            else:\n",
    "                print(f\"No particles accepted at epsilon = {epsilon}. Taking best particles.\")\n",
    "                sorted_indices = np.argsort(distances)\n",
    "                for i in range(max(1, int(n_particles * 0.1))):\n",
    "                    new_weights[sorted_indices[i]] = 1.0\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            \n",
    "            # calculate effective sample size\n",
    "            ESS = 1.0 / np.sum(new_weights**2)\n",
    "            print(f\"Effective sample size: {ESS:.2f}\")\n",
    "            \n",
    "            # resample if needed\n",
    "            if ESS < n_particles / 2 or t == n_populations - 1:\n",
    "                # resample based on weights\n",
    "                indices = np.random.choice(n_particles, size=n_particles, p=new_weights)\n",
    "                resampled_particles = [particles[i] for i in indices]\n",
    "                resampled_trajectories = [trajectories[i] for i in indices]\n",
    "                particles = resampled_particles\n",
    "                trajectories = resampled_trajectories\n",
    "                weights = np.ones(n_particles) / n_particles\n",
    "            else:\n",
    "                weights = new_weights\n",
    "            \n",
    "            # if not final iteration, perturb particles\n",
    "            if t < n_populations - 1:\n",
    "                # calculate kernel covariance\n",
    "                alpha_values = np.array([p[\"alpha\"] for p in particles])\n",
    "                lmbd_values = np.array([p[\"lmbd\"] for p in particles])\n",
    "                \n",
    "                params = np.vstack([alpha_values, lmbd_values]).T\n",
    "                cov = np.cov(params.T) + np.eye(2) * 1e-6  # add small diagonal for stability\n",
    "                \n",
    "                # perturb particles\n",
    "                new_particles = []\n",
    "                for i, particle in enumerate(particles):\n",
    "                    accepted = False\n",
    "                    attempts = 0\n",
    "                    while not accepted and attempts < 100:\n",
    "                        attempts += 1\n",
    "                        # multivariate normal perturbation\n",
    "                        perturbation = multivariate_normal.rvs(mean=[0, 0], cov=cov)\n",
    "                        new_alpha = particle[\"alpha\"] + perturbation[0]\n",
    "                        new_lmbd = particle[\"lmbd\"] + perturbation[1]\n",
    "                        \n",
    "                        # ensure being within history matching bounds\n",
    "                        alpha_in_bounds = alpha_min <= new_alpha <= alpha_max\n",
    "                        lmbd_in_bounds = lmbd_min <= new_lmbd <= lmbd_max\n",
    "                        \n",
    "                        if alpha_in_bounds and lmbd_in_bounds:\n",
    "                            accepted = True\n",
    "                            new_particles.append({\"alpha\": new_alpha, \"lmbd\": new_lmbd})\n",
    "                    \n",
    "                    # if couldn't generate valid particle after max attempts, keep original\n",
    "                    if not accepted:\n",
    "                        new_particles.append(particle)\n",
    "                \n",
    "                particles = new_particles\n",
    "        \n",
    "        # return final particles and weights\n",
    "        final_results = []\n",
    "        for i, particle in enumerate(particles):\n",
    "            final_results.append({\n",
    "                \"alpha\": particle[\"alpha\"],\n",
    "                \"lmbd\": particle[\"lmbd\"],\n",
    "                \"weight\": weights[i],\n",
    "                \"distance\": distances[i],\n",
    "                \"trajectory\": trajectories[i]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(final_results)\n",
    "    \n",
    "    def plot_results(self, results_df, method_name=\"ABC\", n_trajectories=5):\n",
    "        \"\"\"Plot parameter posterior and time series comparison\"\"\"\n",
    "        #import arviz as az\n",
    "    \n",
    "        if len(results_df) == 0:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.text(0.5, 0.5, f\"No accepted parameter sets for {method_name}\", \n",
    "               horizontalalignment='center', verticalalignment='center')\n",
    "            return fig\n",
    "    \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "        # 1. Parameter distribution plot (scatter)\n",
    "        if 'weight' in results_df.columns:\n",
    "            # for SMC with weights\n",
    "            scatter = axes[0].scatter(results_df[\"alpha\"], results_df[\"lmbd\"], \n",
    "                               s=results_df[\"weight\"]*100, alpha=0.6)\n",
    "        else:\n",
    "            # for methods without weights\n",
    "            scatter = axes[0].scatter(results_df[\"alpha\"], results_df[\"lmbd\"], alpha=0.6)\n",
    "    \n",
    "        axes[0].set_title(f\"Parameter posterior - {method_name}\")\n",
    "        axes[0].set_xlabel(\"Alpha\")\n",
    "        axes[0].set_ylabel(\"Lambda\")\n",
    "    \n",
    "        # 2. Trajectories plot\n",
    "        # trajectories from results\n",
    "        n_plot = min(n_trajectories, len(results_df))\n",
    "        for i in range(n_plot):\n",
    "            traj = results_df.iloc[i].get(\"trajectory\")\n",
    "            if traj is not None:\n",
    "                axes[1].plot(traj, alpha=0.6, label=f\"Sim {i+1}\")\n",
    "    \n",
    "        # observed data\n",
    "        axes[1].plot(self.observed_data[\"H1N1\"], color=\"black\", linestyle=\"--\", \n",
    "                linewidth=2, label=\"Observed\")\n",
    "    \n",
    "        axes[1].set_title(\"Time series comparison\")\n",
    "        axes[1].set_xlabel(\"Time\")\n",
    "        axes[1].set_ylabel(\"Infected\")\n",
    "        axes[1].legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(alpha=0.78, lmbd=0.4, days=range(1, 100)):\n",
    "    \"\"\"\n",
    "    Synthetic epidemic data with known parameters as a 'real data'\n",
    "    \"\"\"\n",
    "    pool = Main(\n",
    "        strains_keys=['H1N1', 'H3N2', 'B'],\n",
    "        infected_init=[10, 0, 0],\n",
    "        alpha=[alpha, alpha, alpha],\n",
    "        lmbd=lmbd\n",
    "    )\n",
    "    \n",
    "    pool.runs_params(\n",
    "        num_runs=1,\n",
    "        days=[1, len(days)],\n",
    "        data_folder='chelyabinsk_10'\n",
    "    )\n",
    "    \n",
    "    pool.age_groups_params(\n",
    "        age_groups=['0-10', '11-17', '18-59', '60-150'],\n",
    "        vaccined_fraction=[0, 0, 0, 0]\n",
    "    )\n",
    "    \n",
    "    pool.start(with_seirb=True)\n",
    "    \n",
    "    results_path = os.path.join(pool.results_dir, \"prevalence_seed_0.csv\")\n",
    "    data = pd.read_csv(results_path, sep='\\t')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter ranges for priors\n",
    "prior_ranges = {\n",
    "    \"alpha\": (0.1, 0.9),   # Susceptibility\n",
    "    \"lmbd\": (0.1, 0.9)     # Transmissibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data with known parameters\n",
    "true_alpha = 0.78\n",
    "true_lmbd = 0.4\n",
    "observed_data = generate_synthetic_data(alpha=true_alpha, lmbd=true_lmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b34aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ABM instance\n",
    "abm = ABM_tuned(observed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run history matching\n",
    "hm_results = abm.history_matching(prior_ranges, n_samples=500, epsilon=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26348a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hm_results))\n",
    "abm.plot_results(hm_results, \"History Matching\", n_trajectories=len(hm_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ace845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC Rejection\n",
    "rejection_results = abm.rejection_abc(n_samples=500, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rejection_results))\n",
    "abm.plot_results(rejection_results, \"ABC Rejection\", len(rejection_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ecfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC Annealing\n",
    "annealing_results = abm.annealing_abc(n_samples=500, initial_epsilon=1e-1, final_epsilon=1e-5, cooling_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(annealing_results))\n",
    "abm.plot_results(annealing_results, \"ABC Annealing\", len(annealing_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ABC SMC\n",
    "smc_results = abm.smc_abc(n_particles=500, n_populations=5, initial_epsilon=1e-1, final_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bf1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(smc_results))\n",
    "abm.plot_results(smc_results, \"ABC SMC\", len(smc_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
