{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edaf70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import uniform, norm, multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612838cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSEIR_tuned:\n",
    "    \"\"\"\n",
    "    Network-based SEIR model parameter estimation class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observed_data, network_params=None, fixed_alpha=0.2, fixed_gamma=0.1):\n",
    "        \"\"\"Initialization with fixed alpha and gamma\"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        \n",
    "        # fixed parameters\n",
    "        self.fixed_alpha = fixed_alpha \n",
    "        self.fixed_gamma = fixed_gamma \n",
    "        \n",
    "        # default network parameters\n",
    "        if network_params is None:\n",
    "            network_params = {\n",
    "                'n_nodes': 1000,\n",
    "                'network_type': 'barabasi_albert',\n",
    "                'network_params': {'m': 3}\n",
    "            }\n",
    "        self.network_params = network_params\n",
    "        \n",
    "        # store history matching results\n",
    "        self.hm_results = None\n",
    "        \n",
    "        print(f\"Fixed parameters: alpha = {self.fixed_alpha}, gamma = {self.fixed_gamma}\")\n",
    "        print(\"Calibrating parameters: tau (transmission rate), rho (initial infection fraction)\")\n",
    "    \n",
    "    def generate_network(self):\n",
    "        \"\"\"\n",
    "        Generate network based on specified parameters\n",
    "        \"\"\"\n",
    "        n_nodes = self.network_params['n_nodes']\n",
    "        network_type = self.network_params['network_type']\n",
    "        net_params = self.network_params['network_params']\n",
    "        \n",
    "        if network_type == 'barabasi_albert':\n",
    "            return nx.barabasi_albert_graph(n_nodes, net_params['m'])\n",
    "        elif network_type == 'erdos_renyi':\n",
    "            return nx.erdos_renyi_graph(n_nodes, net_params['p'])\n",
    "        elif network_type == 'watts_strogatz':\n",
    "            return nx.watts_strogatz_graph(n_nodes, net_params['k'], net_params['p'])\n",
    "        elif network_type == 'complete':\n",
    "            return nx.complete_graph(n_nodes)\n",
    "        elif network_type == 'regular':\n",
    "            return nx.random_regular_graph(net_params['d'], n_nodes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown network type: {network_type}\")\n",
    "    \n",
    "    def SEIR_network(self, G, tau, alpha, gamma, rho, tmax):\n",
    "        \"\"\"\n",
    "        SEIR network simulation\n",
    "        \"\"\"\n",
    "        # initialize states: S=0 (Susceptible), E=1 (Exposed), I=2 (Infected), R=3 (Recovered)\n",
    "        for node in G.nodes():\n",
    "            G.nodes[node]['state'] = 0  # start all nodes as susceptible\n",
    "\n",
    "        initial_infected = int(rho * len(G.nodes()))\n",
    "        initial_infected_nodes = random.sample(list(G.nodes()), initial_infected)\n",
    "        for node in initial_infected_nodes:\n",
    "            G.nodes[node]['state'] = 2\n",
    "\n",
    "        susceptible_count = []\n",
    "        exposed_count = []\n",
    "        infected_count = []\n",
    "        recovered_count = []\n",
    "\n",
    "        for day in range(tmax + 1):\n",
    "            new_states = {}\n",
    "\n",
    "            # Count current states\n",
    "            susceptible = sum(1 for n in G.nodes if G.nodes[n]['state'] == 0)\n",
    "            exposed = sum(1 for n in G.nodes if G.nodes[n]['state'] == 1)\n",
    "            infected = sum(1 for n in G.nodes if G.nodes[n]['state'] == 2)\n",
    "            recovered = sum(1 for n in G.nodes if G.nodes[n]['state'] == 3)\n",
    "\n",
    "            susceptible_count.append(susceptible)\n",
    "            exposed_count.append(exposed)\n",
    "            infected_count.append(infected)\n",
    "            recovered_count.append(recovered)\n",
    "\n",
    "            for node in G.nodes():\n",
    "                if G.nodes[node]['state'] == 2:  # infected node\n",
    "                    for neighbor in G.neighbors(node):\n",
    "                        if G.nodes[neighbor]['state'] == 0:  # susceptible neighbor\n",
    "                            if random.random() < tau:\n",
    "                                new_states[neighbor] = 1  # expose neighbor\n",
    "                    if random.random() < gamma:\n",
    "                        new_states[node] = 3  # recover\n",
    "\n",
    "                elif G.nodes[node]['state'] == 1:  # exposed node\n",
    "                    if random.random() < alpha:\n",
    "                        new_states[node] = 2  # become infected\n",
    "\n",
    "            # apply state transitions\n",
    "            for node, new_state in new_states.items():\n",
    "                G.nodes[node]['state'] = new_state\n",
    "\n",
    "        return [index for index in range(tmax + 1)], susceptible_count, exposed_count, infected_count, recovered_count\n",
    "    \n",
    "    def simulator_function(self, tau, rho):\n",
    "        \"\"\"\n",
    "        Run SEIR network simulation with given tau, rho and fixed alpha, gamma\n",
    "        \"\"\"\n",
    "        try:\n",
    "            G = self.generate_network()\n",
    "            tmax = len(self.observed_data) - 1\n",
    "            \n",
    "            # use fixed alpha and gamma\n",
    "            days, S, E, I, R = self.SEIR_network(G, tau, self.fixed_alpha, self.fixed_gamma, rho, tmax)\n",
    "            \n",
    "            sim_results = pd.DataFrame({\n",
    "                'day': days,\n",
    "                'S': S,\n",
    "                'E': E,\n",
    "                'I': I,\n",
    "                'R': R\n",
    "            })\n",
    "            \n",
    "            return sim_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_distance(self, sim_data):\n",
    "        \"\"\"\n",
    "        Calculate distance between simulated and observed data\n",
    "        \"\"\"\n",
    "        if sim_data is None:\n",
    "            return np.inf\n",
    "        \n",
    "        try:\n",
    "            min_len = min(len(self.observed_data), len(sim_data))\n",
    "            obs = self.observed_data['I'].values[:min_len]  # focus on infected compartment\n",
    "            sim = sim_data['I'].values[:min_len]\n",
    "            \n",
    "            # mean squared error\n",
    "            distance = np.mean((obs - sim)**2)\n",
    "            return distance\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance: {e}\")\n",
    "            return np.inf\n",
    "    \n",
    "    def history_matching(self, prior_ranges, n_samples=100, epsilon=1000, adaptive=False, accept_ratio=0.2):\n",
    "        \"\"\"\n",
    "        History matching to find plausible parameter regions for tau and rho only\n",
    "        \"\"\"\n",
    "        print(f\"Running history matching with {n_samples} samples...\")\n",
    "        print(f\"Calibrating: tau, rho | Fixed: alpha={self.fixed_alpha}, gamma={self.fixed_gamma}\")\n",
    "\n",
    "        # generate samples from prior ranges (only tau and rho)\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = {}\n",
    "            for param, (min_val, max_val) in prior_ranges.items():\n",
    "                if param in ['tau', 'rho']:  # only sample tau and rho\n",
    "                    sample[param] = uniform.rvs(loc=min_val, scale=max_val-min_val)\n",
    "            samples.append(sample)\n",
    "\n",
    "        # run simulations and calculate distances\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.simulator_function(\n",
    "                sample[\"tau\"], sample[\"rho\"]\n",
    "            )\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "            \n",
    "            # store trajectory data\n",
    "            result_dict = {\n",
    "                \"tau\": sample[\"tau\"],\n",
    "                \"rho\": sample[\"rho\"],\n",
    "                \"alpha\": self.fixed_alpha,  # store fixed values for reference\n",
    "                \"gamma\": self.fixed_gamma,\n",
    "                \"distance\": distance\n",
    "            }\n",
    "            \n",
    "            # add trajectory to results dictionary\n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"I\"].copy()\n",
    "            \n",
    "            results.append(result_dict)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # filter results\n",
    "        if adaptive:  # by acceptance ratio\n",
    "            n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "            accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "        else:  # fixed threshold\n",
    "            accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "\n",
    "        print(f\"Accepted {len(accepted)} parameter sets\")\n",
    "        \n",
    "        # store results for other methods to use\n",
    "        self.hm_results = accepted\n",
    "\n",
    "        return accepted\n",
    "    \n",
    "    def rejection_abc(self, n_samples=100, epsilon=1e-5, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\"\n",
    "        ABC rejection sampling based on history matching results for tau and rho\n",
    "        \"\"\"\n",
    "        if self.hm_results is None or len(self.hm_results) < 1:\n",
    "            raise ValueError(\"Must run history_matching before rejection_abc and obtain at least one parameter set.\")\n",
    "        \n",
    "        print(f\"Running ABC rejection with {n_samples} samples from History Matching...\")\n",
    "        \n",
    "        # use history matching results to define parameter ranges (only tau and rho)\n",
    "        param_bounds = {}\n",
    "        for param in ['tau', 'rho']:\n",
    "            if param in self.hm_results.columns:\n",
    "                param_bounds[param] = (self.hm_results[param].min(), self.hm_results[param].max())\n",
    "        \n",
    "        # sample from history matching parameter space\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # randomly select a parameter set from history matching results\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            # add small perturbation to create a new sample\n",
    "            sample = {}\n",
    "            for param in param_bounds.keys():\n",
    "                perturb = uniform.rvs(loc=-0.02, scale=0.04)  # +- 0.02\n",
    "                param_min, param_max = param_bounds[param]\n",
    "                sample[param] = np.clip(hm_sample[param] + perturb, param_min, param_max)\n",
    "            \n",
    "            samples.append(sample)\n",
    "        \n",
    "        # run simulations and calculate distances\n",
    "        results = []\n",
    "        for sample in tqdm(samples):\n",
    "            sim_data = self.simulator_function(\n",
    "                sample[\"tau\"], sample[\"rho\"]\n",
    "            )\n",
    "            distance = self.calculate_distance(sim_data)\n",
    "            \n",
    "            result_dict = {\n",
    "                \"tau\": sample[\"tau\"],\n",
    "                \"rho\": sample[\"rho\"],\n",
    "                \"alpha\": self.fixed_alpha,\n",
    "                \"gamma\": self.fixed_gamma,\n",
    "                \"distance\": distance\n",
    "            }\n",
    "            \n",
    "            if sim_data is not None:\n",
    "                result_dict[\"trajectory\"] = sim_data[\"I\"].copy()\n",
    "            \n",
    "            results.append(result_dict)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if adaptive:  # by acceptance ratio\n",
    "            n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "            accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "        else:  # fixed threshold\n",
    "            accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "            if len(accepted) == 0:\n",
    "                print(f\"No samples accepted at epsilon = {epsilon}. Taking 10% of the best samples.\")\n",
    "                accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "        \n",
    "        print(f\"Accepted {len(accepted)} parameter sets from {n_samples} samples\")\n",
    "        return accepted\n",
    "    \n",
    "    def annealing_abc(self, n_samples=100, initial_epsilon=1e-3, final_epsilon=1e-5, cooling_steps=3, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\"\n",
    "        ABC simulated annealing using history matching results for tau and rho\n",
    "        \"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"Must run history_matching before annealing_abc\")\n",
    "        \n",
    "        print(f\"Running ABC annealing with {cooling_steps} cooling steps...\")\n",
    "        \n",
    "        # epsilon values for each step\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, cooling_steps)\n",
    "        \n",
    "        # parameter bounds from history matching (only tau and rho)\n",
    "        param_bounds = {}\n",
    "        for param in ['tau', 'rho']:\n",
    "            if param in self.hm_results.columns:\n",
    "                param_bounds[param] = (self.hm_results[param].min(), self.hm_results[param].max())\n",
    "        \n",
    "        # initial samples from history matching results\n",
    "        current_samples = []\n",
    "        for _ in range(n_samples):\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            sample = {}\n",
    "            for param in param_bounds.keys():\n",
    "                sample[param] = hm_sample[param]\n",
    "            current_samples.append(sample)\n",
    "        \n",
    "        # annealing process\n",
    "        for step, epsilon in enumerate(epsilons):\n",
    "            print(f\"Annealing step {step+1}/{cooling_steps}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate current samples\n",
    "            results = []\n",
    "            for sample in tqdm(current_samples):\n",
    "                sim_data = self.simulator_function(\n",
    "                    sample[\"tau\"], sample[\"rho\"]\n",
    "                )\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                \n",
    "                result_dict = {\n",
    "                    \"tau\": sample[\"tau\"],\n",
    "                    \"rho\": sample[\"rho\"],\n",
    "                    \"alpha\": self.fixed_alpha,\n",
    "                    \"gamma\": self.fixed_gamma,\n",
    "                    \"distance\": distance\n",
    "                }\n",
    "                \n",
    "                if sim_data is not None:\n",
    "                    result_dict[\"trajectory\"] = sim_data[\"I\"].copy()\n",
    "                \n",
    "                results.append(result_dict)\n",
    "            \n",
    "            # filter accepted samples\n",
    "            results_df = pd.DataFrame(results)\n",
    "            \n",
    "            if adaptive:  # by acceptance ratio\n",
    "                n_accept = max(1, int(len(results_df) * accept_ratio))\n",
    "                accepted = results_df.nsmallest(n_accept, \"distance\")\n",
    "            else:  # fixed threshold\n",
    "                accepted = results_df[results_df[\"distance\"] < epsilon]\n",
    "                if len(accepted) == 0:\n",
    "                    print(f\"No samples accepted at epsilon = {epsilon}. Taking 10% of the best samples.\")\n",
    "                    accepted = results_df.nsmallest(max(1, int(n_samples * 0.1)), \"distance\")\n",
    "            \n",
    "            # generate new samples for next iteration\n",
    "            if step < cooling_steps - 1:\n",
    "                # calculate means and variances of accepted parameters\n",
    "                new_samples = []\n",
    "                for _ in range(n_samples):\n",
    "                    sample = {}\n",
    "                    for param in param_bounds.keys():\n",
    "                        param_mean = accepted[param].mean()\n",
    "                        param_var = accepted[param].var()\n",
    "                        \n",
    "                        if np.isnan(param_var) or param_var <= 1e-6:\n",
    "                            param_var = 1e-4\n",
    "                        \n",
    "                        try:\n",
    "                            new_value = norm.rvs(loc=param_mean, scale=np.sqrt(param_var))\n",
    "                        except ValueError:\n",
    "                            # fallback\n",
    "                            perturb_scale = 0.05\n",
    "                            param_min, param_max = param_bounds[param]\n",
    "                            new_value = param_mean + np.random.uniform(-perturb_scale, perturb_scale) * (param_max - param_min)\n",
    "                        \n",
    "                        # ensure within bounds\n",
    "                        param_min, param_max = param_bounds[param]\n",
    "                        sample[param] = max(param_min, min(new_value, param_max))\n",
    "                    \n",
    "                    new_samples.append(sample)\n",
    "                \n",
    "                current_samples = new_samples\n",
    "        \n",
    "        return accepted\n",
    "    \n",
    "    def smc_abc(self, n_particles=100, n_populations=3, initial_epsilon=1e-3, final_epsilon=1e-5, adaptive=True, accept_ratio=0.01):\n",
    "        \"\"\"\n",
    "        ABC Sequential Monte Carlo using history matching results for tau and rho\n",
    "        \"\"\"\n",
    "        if self.hm_results is None:\n",
    "            raise ValueError(\"Must run history_matching before smc_abc\")\n",
    "        \n",
    "        print(f\"Running ABC-SMC with {n_populations} populations...\")\n",
    "        \n",
    "        # epsilon sequence\n",
    "        epsilons = np.geomspace(initial_epsilon, final_epsilon, n_populations)\n",
    "        \n",
    "        # parameter bounds from history matching (only tau and rho)\n",
    "        param_bounds = {}\n",
    "        for param in ['tau', 'rho']:\n",
    "            if param in self.hm_results.columns:\n",
    "                param_bounds[param] = (self.hm_results[param].min(), self.hm_results[param].max())\n",
    "        \n",
    "        # first population from history matching results\n",
    "        particles = []\n",
    "        for _ in range(n_particles):\n",
    "            hm_idx = np.random.randint(0, len(self.hm_results))\n",
    "            hm_sample = self.hm_results.iloc[hm_idx]\n",
    "            \n",
    "            particle = {}\n",
    "            for param in param_bounds.keys():\n",
    "                particle[param] = hm_sample[param]\n",
    "            particles.append(particle)\n",
    "        \n",
    "        # equal weights for first population\n",
    "        weights = np.ones(n_particles) / n_particles\n",
    "        \n",
    "        # SMC process\n",
    "        for t in range(n_populations):\n",
    "            epsilon = epsilons[t]\n",
    "            print(f\"SMC Population {t+1}/{n_populations}, epsilon = {epsilon:.6f}\")\n",
    "            \n",
    "            # evaluate particles and calculate distances\n",
    "            distances = []\n",
    "            trajectories = []\n",
    "            for particle in tqdm(particles):\n",
    "                sim_data = self.simulator_function(\n",
    "                    particle[\"tau\"], particle[\"rho\"]\n",
    "                )\n",
    "                distance = self.calculate_distance(sim_data)\n",
    "                distances.append(distance)\n",
    "                if sim_data is not None:\n",
    "                    trajectories.append(sim_data[\"I\"].copy())\n",
    "                else:\n",
    "                    trajectories.append(None)\n",
    "            \n",
    "            # update weights based on epsilon\n",
    "            new_weights = np.zeros(n_particles)\n",
    "            for i, distance in enumerate(distances):\n",
    "                if distance < epsilon:\n",
    "                    new_weights[i] = weights[i]\n",
    "            \n",
    "            # normalize weights\n",
    "            if np.sum(new_weights) > 0:\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            else:\n",
    "                print(f\"No particles accepted at epsilon = {epsilon}. Taking best particles.\")\n",
    "                sorted_indices = np.argsort(distances)\n",
    "                for i in range(max(1, int(n_particles * 0.1))):\n",
    "                    new_weights[sorted_indices[i]] = 1.0\n",
    "                new_weights = new_weights / np.sum(new_weights)\n",
    "            \n",
    "            # calculate effective sample size\n",
    "            ESS = 1.0 / np.sum(new_weights**2)\n",
    "            print(f\"Effective sample size: {ESS:.2f}\")\n",
    "            \n",
    "            # resample if needed\n",
    "            if ESS < n_particles / 2 or t == n_populations - 1:\n",
    "                indices = np.random.choice(n_particles, size=n_particles, p=new_weights)\n",
    "                resampled_particles = [particles[i] for i in indices]\n",
    "                resampled_trajectories = [trajectories[i] for i in indices]\n",
    "                particles = resampled_particles\n",
    "                trajectories = resampled_trajectories\n",
    "                weights = np.ones(n_particles) / n_particles\n",
    "            else:\n",
    "                weights = new_weights\n",
    "            \n",
    "            # if not final iteration, perturb particles\n",
    "            if t < n_populations - 1:\n",
    "                # calculate kernel covariance (only for tau and rho)\n",
    "                param_values = []\n",
    "                for param in param_bounds.keys():\n",
    "                    param_values.append([p[param] for p in particles])\n",
    "                \n",
    "                params = np.array(param_values).T\n",
    "                cov = np.cov(params.T) + np.eye(len(param_bounds)) * 1e-6\n",
    "                \n",
    "                # perturb particles\n",
    "                new_particles = []\n",
    "                for i, particle in enumerate(particles):\n",
    "                    accepted = False\n",
    "                    attempts = 0\n",
    "                    while not accepted and attempts < 100:\n",
    "                        attempts += 1\n",
    "                        # multivariate normal perturbation\n",
    "                        perturbation = multivariate_normal.rvs(mean=np.zeros(len(param_bounds)), cov=cov)\n",
    "                        \n",
    "                        new_particle = {}\n",
    "                        valid = True\n",
    "                        for j, param in enumerate(param_bounds.keys()):\n",
    "                            new_value = particle[param] + perturbation[j]\n",
    "                            param_min, param_max = param_bounds[param]\n",
    "                            if param_min <= new_value <= param_max:\n",
    "                                new_particle[param] = new_value\n",
    "                            else:\n",
    "                                valid = False\n",
    "                                break\n",
    "                        \n",
    "                        if valid:\n",
    "                            accepted = True\n",
    "                            new_particles.append(new_particle)\n",
    "                    \n",
    "                    if not accepted:\n",
    "                        new_particles.append(particle)\n",
    "                \n",
    "                particles = new_particles\n",
    "        \n",
    "        # return final particles and weights\n",
    "        final_results = []\n",
    "        for i, particle in enumerate(particles):\n",
    "            final_results.append({\n",
    "                \"tau\": particle[\"tau\"],\n",
    "                \"rho\": particle[\"rho\"],\n",
    "                \"alpha\": self.fixed_alpha,\n",
    "                \"gamma\": self.fixed_gamma,\n",
    "                \"weight\": weights[i],\n",
    "                \"distance\": distances[i],\n",
    "                \"trajectory\": trajectories[i]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(final_results)\n",
    "    \n",
    "    def plot_results(self, results_df, method_name=\"ABC\", n_trajectories=5):\n",
    "        \"\"\"\n",
    "        Plot parameter posterior for tau and rho (2D) with time series comparison\n",
    "        \"\"\"\n",
    "        if len(results_df) == 0:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.text(0.5, 0.5, f\"No accepted parameter sets for {method_name}\", \n",
    "                   horizontalalignment='center', verticalalignment='center')\n",
    "            return fig\n",
    "\n",
    "        # debug information\n",
    "        print(f\"Plotting {len(results_df)} parameter sets for {method_name}\")\n",
    "        print(\"Parameter ranges:\")\n",
    "        for param in ['tau', 'rho']:\n",
    "            if param in results_df.columns:\n",
    "                print(f\"  {param}: {results_df[param].min():.6f} - {results_df[param].max():.6f}\")\n",
    "        print(f\"Fixed: alpha = {self.fixed_alpha}, gamma = {self.fixed_gamma}\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "        # 1. 2D Parameter posterior plot (tau vs rho)\n",
    "        ax1 = fig.add_subplot(2, 2, 1)\n",
    "\n",
    "        # add small jitter to prevent overlapping points\n",
    "        jitter_amount = 0.001\n",
    "        tau_jitter = results_df[\"tau\"] + np.random.normal(0, jitter_amount, len(results_df))\n",
    "        rho_jitter = results_df[\"rho\"] + np.random.normal(0, jitter_amount/10, len(results_df))\n",
    "\n",
    "        if 'weight' in results_df.columns:\n",
    "            scatter = ax1.scatter(tau_jitter, rho_jitter,\n",
    "                                s=results_df[\"weight\"]*200 + 20,  # marker size based on weights\n",
    "                                c=results_df[\"distance\"],  # color based on distance\n",
    "                                cmap='viridis_r', alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "            cbar = fig.colorbar(scatter, ax=ax1)\n",
    "            cbar.set_label('Distance')\n",
    "        else:\n",
    "            scatter = ax1.scatter(tau_jitter, rho_jitter,\n",
    "                                c=results_df[\"distance\"], cmap='viridis_r', alpha=0.7, s=80,\n",
    "                                edgecolors='black', linewidth=0.5)\n",
    "            cbar = fig.colorbar(scatter, ax=ax1)\n",
    "            cbar.set_label('Distance')\n",
    "\n",
    "        ax1.set_xlabel('Tau (transmission rate)', fontsize=12)\n",
    "        ax1.set_ylabel('Rho (initial infection fraction)', fontsize=12)\n",
    "        ax1.set_title(f'Parameter Posterior - {method_name}\\n({len(results_df)} points)\\nFixed: α={self.fixed_alpha}, γ={self.fixed_gamma}', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "\n",
    "        # 2. Tau distribution\n",
    "        ax2 = fig.add_subplot(2, 2, 2)\n",
    "        \n",
    "        bins = max(15, min(30, len(results_df)//2))\n",
    "        \n",
    "        if 'weight' in results_df.columns:\n",
    "            ax2.hist(results_df[\"tau\"], bins=bins, alpha=0.7, \n",
    "                    weights=results_df[\"weight\"], density=True, \n",
    "                    edgecolor='black', color='blue', label='Tau')\n",
    "        else:\n",
    "            ax2.hist(results_df[\"tau\"], bins=bins, alpha=0.7, \n",
    "                    density=True, edgecolor='black', color='blue', label='Tau')\n",
    "\n",
    "        ax2.set_title(f\"Tau distribution\\n({len(results_df)} samples)\")\n",
    "        ax2.set_xlabel(\"Tau (transmission rate)\")\n",
    "        ax2.set_ylabel(\"Density\")\n",
    "        ax2.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
    "\n",
    "        # 3. Rho distribution\n",
    "        ax3 = fig.add_subplot(2, 2, 3)\n",
    "        \n",
    "        if 'weight' in results_df.columns:\n",
    "            ax3.hist(results_df[\"rho\"], bins=bins, alpha=0.7, \n",
    "                    weights=results_df[\"weight\"], density=True, \n",
    "                    edgecolor='black', color='red', label='Rho')\n",
    "        else:\n",
    "            ax3.hist(results_df[\"rho\"], bins=bins, alpha=0.7, \n",
    "                    density=True, edgecolor='black', color='red', label='Rho')\n",
    "\n",
    "        ax3.set_title(f\"Rho distribution\\n({len(results_df)} samples)\")\n",
    "        ax3.set_xlabel(\"Rho (initial infection fraction)\")\n",
    "        ax3.set_ylabel(\"Density\")\n",
    "        ax3.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
    "\n",
    "        # 4. Time series comparison\n",
    "        ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "        n_plot = min(n_trajectories, len(results_df))\n",
    "        if n_plot > 0:\n",
    "            if 'weight' in results_df.columns:\n",
    "                sorted_indices = results_df['weight'].nlargest(n_plot).index\n",
    "            else:\n",
    "                sorted_indices = results_df['distance'].nsmallest(n_plot).index\n",
    "\n",
    "            traj_num = 0\n",
    "            for i, idx in enumerate(sorted_indices):\n",
    "                traj = results_df.loc[idx].get(\"trajectory\")\n",
    "                if traj is not None:\n",
    "                    alpha_val = 0.8 if i < 3 else 0.4\n",
    "                    if traj_num <10:\n",
    "                        ax4.plot(traj, alpha=alpha_val, label=f\"Sim {i+1}\", linewidth=1.5)\n",
    "                    else:\n",
    "                        ax4.plot(traj, alpha=alpha_val, linewidth=1.5, label='')\n",
    "                    traj_num += 1\n",
    "\n",
    "        ax4.plot(self.observed_data[\"I\"], color=\"black\", linestyle=\"--\", \n",
    "                linewidth=3, label=\"Observed\")\n",
    "\n",
    "        ax4.set_title(\"Time series comparison\")\n",
    "        ax4.set_xlabel(\"Time\")\n",
    "        ax4.set_ylabel(\"Infected\")\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def debug_results(self, results_df, method_name=\"ABC\"):\n",
    "        \"\"\"\n",
    "        Debug function to analyze the parameter distributions\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== DEBUG INFO for {method_name} ===\")\n",
    "        print(f\"Number of accepted parameter sets: {len(results_df)}\")\n",
    "        print(f\"Fixed parameters: alpha = {self.fixed_alpha}, gamma = {self.fixed_gamma}\")\n",
    "        \n",
    "        if len(results_df) == 0:\n",
    "            print(\"No parameter sets to analyze!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nCalibrated parameter statistics:\")\n",
    "        for param in ['tau', 'rho', 'distance']:\n",
    "            if param in results_df.columns:\n",
    "                values = results_df[param]\n",
    "                print(f\"{param}:\")\n",
    "                print(f\"  Min: {values.min():.8f}\")\n",
    "                print(f\"  Max: {values.max():.8f}\")\n",
    "                print(f\"  Mean: {values.mean():.8f}\")\n",
    "                print(f\"  Std: {values.std():.8f}\")\n",
    "                print(f\"  Range: {values.max() - values.min():.8f}\")\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        duplicates = results_df.duplicated(['tau', 'rho']).sum()\n",
    "        print(f\"\\nDuplicate parameter sets: {duplicates}\")\n",
    "        \n",
    "        if 'weight' in results_df.columns:\n",
    "            print(f\"\\nWeight statistics:\")\n",
    "            print(f\"  Min weight: {results_df['weight'].min():.8f}\")\n",
    "            print(f\"  Max weight: {results_df['weight'].max():.8f}\")\n",
    "            print(f\"  Mean weight: {results_df['weight'].mean():.8f}\")\n",
    "\n",
    "\n",
    "def generate_synthetic_seir_data(tau=0.3, alpha=0.2, gamma=0.1, rho=0.01, tmax=99, network_params=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic SEIR epidemic data with known parameters\n",
    "    \"\"\"\n",
    "    if network_params is None:\n",
    "        network_params = {\n",
    "            'n_nodes': 1000,\n",
    "            'network_type': 'barabasi_albert',\n",
    "            'network_params': {'m': 3}\n",
    "        }\n",
    "    \n",
    "    # create dummy instance to generate network\n",
    "    dummy_seir = NetworkSEIR_tuned(pd.DataFrame({'I': [0]}), network_params, \n",
    "                                   fixed_alpha=alpha, fixed_gamma=gamma)\n",
    "    G = dummy_seir.generate_network()\n",
    "    \n",
    "    days, S, E, I, R = dummy_seir.SEIR_network(G, tau, alpha, gamma, rho, tmax)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'day': days,\n",
    "        'S': S,\n",
    "        'E': E,\n",
    "        'I': I,\n",
    "        'R': R\n",
    "    })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb25dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter ranges for SEIR model\n",
    "prior_ranges = {\n",
    "        \"tau\": (0.1, 0.9), # transmission rate\n",
    "        \"rho\": (0.001, 0.999) # initial infection fraction\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b2861cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data with known parameters\n",
    "true_tau = 0.3\n",
    "true_alpha = 0.2\n",
    "true_gamma = 0.1\n",
    "true_rho = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b552fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_params = {\n",
    "        'n_nodes': 1000,\n",
    "        'network_type': 'barabasi_albert',\n",
    "        'network_params': {'m': 3}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_data = generate_synthetic_seir_data(\n",
    "        tau=true_tau, alpha=true_alpha, gamma=true_gamma, \n",
    "        rho=true_rho, network_params=network_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24642b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NetworkSEIR instance\n",
    "seir_abc = NetworkSEIR_tuned(observed_data, network_params, fixed_alpha=true_alpha, fixed_gamma=true_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b94a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run history matching\n",
    "print(\"Running history matching...\")\n",
    "hm_results = seir_abc.history_matching(prior_ranges, n_samples=1000, epsilon=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aded41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hm_results))\n",
    "seir_abc.plot_results(hm_results, \"History Matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running ABC rejection...\")\n",
    "rejection_results = seir_abc.rejection_abc(n_samples=50, adaptive=True, accept_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rejection_results))\n",
    "seir_abc.plot_results(rejection_results, \"ABC Rejection\", len(rejection_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a12afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running ABC annealing...\")\n",
    "annealing_results = seir_abc.annealing_abc(n_samples=50, cooling_steps=3, adaptive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(annealing_results))\n",
    "seir_abc.plot_results(annealing_results, \"ABC Annealing\", len(annealing_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d24dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running ABC SMC...\")\n",
    "smc_results = seir_abc.smc_abc(n_particles=500, n_populations=5, adaptive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59095ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(smc_results))\n",
    "seir_abc.plot_results(smc_results, \"ABC SMC\", len(smc_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea94086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
